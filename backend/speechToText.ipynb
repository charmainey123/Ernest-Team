{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0742955",
   "metadata": {},
   "source": [
    "# Importing All Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b01fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "import soundfile as sf\n",
    "import openai\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59ee2a4",
   "metadata": {},
   "source": [
    "# Initializing All Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c44dcf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading API Key\n",
    "with open(\"./apikey.txt\") as file:\n",
    "    openai.api_key = file.read()\n",
    "\n",
    "# Define path to audio output file\n",
    "output_audio_file = \"./output.mp3\"\n",
    "\n",
    "# Initializing text-to-speech object\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Initialize boolean values\n",
    "trigger_ernest = True\n",
    "trigger_response = False\n",
    "\n",
    "# Define the persona of Ernest\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Ernest, a friendly and empathetic virtual avatar from Standard Chartered Bank that can help recommend one of the bank savings account or current account best suited to customers' needs through meaningful conversations. When customers are lost, he can also explain how to fill up the bank account application form.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi Ernest\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"How can I help you?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b999fa",
   "metadata": {},
   "source": [
    "# Define functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8940f991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak(text):\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "    \n",
    "def generate_response(transcript):\n",
    "    \n",
    "    transcript += '\\n\\n===\\n\\n'\n",
    "    # Append to the conversation\n",
    "    messages.append({\"role\": \"user\", \"content\": transcript})\n",
    "    conversation_context = ' '.join([f\"{msg['role']}: {msg['content']}\" for msg in messages])\n",
    "    \n",
    "    # Generate an instance of fine-tuned chatGPT\n",
    "    response = openai.Completion.create(engine='curie:ft-personal-2023-06-04-10-13-42',\n",
    "                                        prompt=conversation_context,\n",
    "                                        temperature=0.7,\n",
    "                                        max_tokens=50,\n",
    "                                        n=1,\n",
    "                                        stop=[\"\\n\"],\n",
    "                                        timeout=None)\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979ecf0c",
   "metadata": {},
   "source": [
    "# Audio-Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21f522ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part of the code is triggered when the user clicks on the button \"speak to Ernest\"\n",
    "\"\"\"\n",
    "\n",
    "def trigger_response():\n",
    "    try:\n",
    "        with sr.Microphone() as mic:\n",
    "            recognizer = sr.Recognizer()\n",
    "            recognizer.adjust_for_ambient_noise(mic, duration = 1)\n",
    "\n",
    "            # Line 13 will be replaced by D-ID audio once it is ready to make the voice quality more consistent\n",
    "            speak(\"How can I help you?\")\n",
    "            audio = recognizer.listen(mic, phrase_time_limit=None, timeout=None)\n",
    "            with open(output_audio_file, \"wb\") as f:\n",
    "                f.write(audio.get_wav_data(convert_rate=44100, convert_width=2))\n",
    "            audio_file = open(output_audio_file, \"rb\")\n",
    "            transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)[\"text\"]\n",
    "            print(\"The transcript is:\", transcript)\n",
    "            response = generate_response(transcript)\n",
    "            print(\"The response is:\", response)\n",
    "\n",
    "            # Line 24 will be replaced by D-ID once it is ready\n",
    "            speak(response)\n",
    "    except sr.UnknownValueError:\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99da9b27",
   "metadata": {},
   "source": [
    "# Call for Ernest (TBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "647ad901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No audio detected. Continue detecting in background...\n",
      "my honest\n",
      "The transcript is: I need help opening a bank account but I'm not sure which one I should open. Could you suggest some products for me?\n",
      "The response is: No problem, which type of account are you looking for?\n",
      "No audio detected. Continue detecting in background...\n",
      "hey dennis\n",
      "hey ernest\n",
      "The transcript is: I'm looking for a current account.\n",
      "The response is: No problem, which Current Account would you like to apply for?\n",
      "pianist\n",
      "heatonist\n",
      "gabe earnest\n",
      "The transcript is: What current accounts do you have to recommend to me?\n",
      "The response is: \n",
      "No audio detected. Continue detecting in background...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7a07c670d087>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mrecognizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRecognizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mrecognizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjust_for_ambient_noise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mduration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0maudio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecognizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlisten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphrase_time_limit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecognizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecognize_google\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\speech_recognition\\__init__.py\u001b[0m in \u001b[0;36mlisten\u001b[1;34m(self, source, timeout, phrase_time_limit, snowboy_configuration)\u001b[0m\n\u001b[0;32m    521\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m                 \u001b[0mbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCHUNK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mbreak\u001b[0m  \u001b[1;31m# reached end of the stream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                 \u001b[0mframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\speech_recognition\\__init__.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyaudio_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexception_on_overflow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyaudio\\__init__.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    568\u001b[0m                 raise IOError(\"Not input stream\",\n\u001b[0;32m    569\u001b[0m                               paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 570\u001b[1;33m             return pa.read_stream(self._stream, num_frames,\n\u001b[0m\u001b[0;32m    571\u001b[0m                                   exception_on_overflow)\n\u001b[0;32m    572\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        with sr.Microphone() as mic:\n",
    "            recognizer = sr.Recognizer()\n",
    "            recognizer.adjust_for_ambient_noise(mic, duration = 0.3)\n",
    "            audio = recognizer.listen(mic, phrase_time_limit=3)\n",
    "            call = recognizer.recognize_google(audio).lower()\n",
    "            print(call)\n",
    "\n",
    "            if 'ernest' in call or 'honest' in call or 'hyannis' in call or 'earnest' in call:\n",
    "                trigger_response()\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"No audio detected. Continue detecting in background...\")\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
